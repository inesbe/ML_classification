{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=black>Entrainer, régler et valider un algorithme de classification avec la bonne méthodologie </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour entrainer un modèle et l’évaluer avec la bonne méthodologie on va voir comment:\n",
    "* Créer un `train_set` et un `test_set`avec la fonction `train_test_split()`\n",
    "* Valider un modèle avec la technique de `cross validation` \n",
    "* Améliorer un modèle en utilisant `GridSerchCV` et `RandomizedSerchCV`   \n",
    "* Valider un modèle avec `learning_curve`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour ce Notebook, nous allons utiliser le célèbre jeu de données `IRIS`. Ce dataset décrit les espèces d’IRIS par quatre propriétés : `longueur` et `largeur` de `sépales` ainsi que `longueur` et `largeur` de `pétales`. IRIS comporte `150` observations (50 observations par espèce)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"petal.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"iris.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue>Entrainement et evaluation d'un modèle de classification</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour commencer on doit  charger les bibliothèques necessaires:\n",
    "* `numpy` : (numerical python)\n",
    "* `sklearn`: Sckit-learn\n",
    "Sklearn vient avec un ensemble de bases de données prêtes à l’emploi pour des raisons académiques(apprendre le machine learning). \n",
    "Ces bases de données sont regroupées dans le package  sklearn.datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Chargement des bibliothèques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "   # un tableau 2D qui contient les mesures de chaque fleur\n",
    "\n",
    "   # un tableau 1D qui contient l'éspèce de chaque fleur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vérifier les types de X et y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoir une idée sur les dimensions de X et y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# afficher les noms des variables (“feature” en anglais) ainsi que les classes des fleurs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant il est temps d’entrainer un modèle machine learning. Pour faire simple, nous allons commencer avec l'un des modèles de classification le plus simple, ici en occurence le KNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chargement du modèle KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainer le modèle avec la méthode fit\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer le score du modèle avec la méthode score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un score de 100%. Qu'en pensez-vous?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En machine learning il ne faut jamais évaluer la performance de votre modèle sur les mêmes données qui vont servir à son entrainement. Donc quand on fait du machine learning on divise toujours notre dataset en deux parties un `train_set` et un `test_set`. En générale on met 80% des données dans le `train_set`et 20% pour le `test_set`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color=red>Train_test_split</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour diviser le dataset on importe  la fonction `tarin_test_split`, ensuite on crée des tableaux `X_train, X_test, y_train, et y_test` et on fait passer les données X et y dans la fonction `train_test_split()`  après, on peut choisir le pourcentage de données à mettre dans le `train_set` et dans le `test_set`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"split.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger la méthode train_test_split qui nous vient du module model_selection de la bibliothèque sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executer le code ci_dessous une autre fois et comparer les valeurs de X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous n’obtenez pas les mêmes nuages c’est normal. Quand on utilise la fonction `train_test_split` le dataset est mélangé de façon aléatoire, avant d’être diviser en deux parties. Si vous désirez contrôler l’aléatoire fixez l’argument `rondom_state`  sur un nombre par exemple 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "                          # n_neighbors=1\n",
    "\n",
    "                          # la méthode fit pour entrainer un modèle\n",
    "\n",
    "                           # la méthode score pour calculer le test_score ou le train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                           # n_neighbors=3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                           # n_neighbors=5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous avez presque tout compris parce qu’il reste une dernière chose à voir. En effet qu’est ce qui nous garantit que la façon dont on découpe le dataset est la bonne? Eh bien face à cette situation il existe une solution c’est la `cross_validation`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>Cross Validation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " La `cross_validation` consite à entrainer puis valider notre modèle sur plusieurs découpes possibles du `train_set`. Par exemple en découpant le `train_set` en 5 parties, on peut entrainer notre modèle sur les 4 premières parties puis le valider sur la cinquième partie. Ensuite, on va refaire tout ça pour toutes les configurations possibles … au final on fera la moyenne des 5 scores et ainsi lorsqu’on voudra comparer nos modèles alors on sera sûr de prendre celui qui a en moyenne eu les meilleures performances! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cross_validation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour faire tout ça dans Python on importe la fonction `cross_val_score`. Dans cette fonction il suffit de faire passer notre modèle nos données `X_train` et `y_train` ainsi que le nombre de split(découpes) que l’on veut dans notre `cross_validation` par exemple 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNeighborsClassifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant on va essayer d'améliorer le modèle et obtenir un score de 95%, 96% voir 99%. Il va falloir donc régler les hyperparamètres du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue>Réglage des Hyperparamètres</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour commencer on va essayer d'optimiser le nombre des voisins 'n_neighbors'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour aller vite on pourrait tester tout ça dans une boucle `for` en enregistrant chaque score que l’on obtient dans une liste `val_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite en affichant ces résultats avec `Matplotlib`, on obtient le graphique ci-dessous (voir graphique) donc on peut voir qu’on obtiendra les meilleurs performances quand on aura des nombres de voisins qui sont aux alentours de 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>Validation curves</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec la fonction validation curve on fait passer notre modèle ainsi que nos données du train set ensuite on indique dans une chaine de caractères (string) le nom de l’hyper paramètre que l’on désire régler puis sous forme d’itérateur on va désigner les différentes valeurs que l’on veut tester pour cette hyper-paramètre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour avoir le score moyen pour chaque cross validation il reste à prendre la moyenne de chaque ligne c’est-à-dire la moyenne suivant l’axe 1 de notre tableau Numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on voit qu’on obtient exactement le même graphique que celui de tout à l’heure!! Mais on plus d’avoir le `validation score` on peut aussi voir le score sur le train set pour ça on pourra compléter notre code en écrivant quelques lignes supplémentaires dans `matplotlib` et ça c’est très utile pour détecter les cas d’overfitting! on peut donc repérer ce problème lorsqu’on a un très bon score à l’entrainement mais un moins bon score sur la validation en l’occurrence sur les algorithmes KNN,  on est très souvent en cas d’overfitting lorsqu’on a un nombre de voisins=1 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vous savez dans l’algorithme KNN il existe d’autres hyper-paramatres que le nombre de voisins! On a par exemple le type de distance entre une distance de manhattan une distance euclidienne on peut aussi choisir d’accorder ou non des coefficients sur nos distances… du coup en réglant ses autres hyper-paramètres, on peut avoir encore une meilleur performance! Alors pour tester toutes ces combinaisons le mieux c’est d’utiliser `GridSerchCV` et `RandomizedSearchCV` ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>GridSearchCV </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GridsearchCV` nous permet de trouver le modèle avec les meilleurs hyper-paramètres en comparent les différentes performances de chaque combinaison grâce à la technique de `cross-validation`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois l’entrainement terminé, on peut voir le modèle qui a obtenu le meilleur score. On obtient 98%. On peut également voir les meilleurs paramètres de ce modèle. On voit donc que le meilleur modèle utilise des distances euclidiennes avec un nombre de voisins =9 et pour finir on peut sauvegarder ce modèle (code) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>RandomizedSearchCV </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue>Evaluer la performance de notre modèle</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ce stade on peut utiliser une autre mesure pour évaluer la performance de notre modèle on peur par exemple importer la fonction confusion matrix qui nous vient du module metrics. Dans cette fonction confusion_matrix on fait passer nos vraies données y_test c’est-à-dire celle qu’on est censées obtenir c’est-à-dire modèle.predict(X_test), on obtient donc une matrice carrée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici la `confusion_matrix` est de dimension $3*3$ puisqu’on a $3$ classes  de fleurs dans notre dataset. On peut voir que les $8$ fleurs  de classe $1$ que l’on retrouve  dans les données de test set ont bien été classées dans cette classe. En revanche parmi les $11$ fleurs de la classe numéro $2$ seulement $9$ d’entre elles ont bien été classées dans la classe $2$ et deux parmi ces $11$ fleurs a été rangée dans la classe numéro $3$. Enfin on voit que dans la classe $3$ on avait également $11$ fleurs et elles ont toutes été bien rangées dans la classe numéro $3$. Donc voilà comment utiliser la fonction `confusion_matrix` qui nous vient du module `model_selection`. C’est une métrique extrêmement utile pour mieux comprendre où sont les erreurs dans votre modèle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red> Learning Curves</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A présent on pourrait se demander si notre modèle pourrait encore avoir des meilleures performances si on lui fournissait plus de données… Pour répondre à cette question très importante il faut tracer ce qu’on appelle les courbes d’apprentissage(`learning curve`).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir que la performance n’évolue presque plus à partir du moment où on a plus de 60 points dans notre `dataset`donc ça nous montre que le modèle va continuer à stagner il est très peu probable qu’on obtienne des meilleures performances en ayant 100 200 1000 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue> Conclusion</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous savez à présent tout ce qu’il y a à savoir concernant l’entrainement, l’optimisation, la validation et l’évaluation finale d’un modèle.  \n",
    "Pour résumer:  \n",
    "- Commencer toujours par télecharger votre `dataset`\n",
    "- Diviser votre dataset par la fonction `train_test_split()`  \n",
    "- Utilisez les optimisateurs `GridSerachCV` et `RandomisedSerachCV`pour trouver les meilleurs hyper paramètres pour votre modèle. \n",
    "- `GridSerachCV`  utilise la `cross_validation` donc pensez à bien définir un nombre de split et optionnellement à définir la stratégie de découpage qui vous intéresse.  \n",
    "- Pensez à utiliser `Validation_curve` et `learning_curve` pour vérifier si vous n’êtes pas en overfitting et pour vérifier si vous pouvez encore améliorer votre modèle avec plus de données."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
